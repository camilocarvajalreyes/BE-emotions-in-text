{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions analysis of written text\n",
    "Author: Camilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pre_trained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: If load pretrained is set to false then be prepared for a long training session (~6 hours for 20000 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set\n",
    "We load the [EmoBank data set](https://github.com/JULIELab/EmoBank)\n",
    "\n",
    "Sven Buechel and Udo Hahn. 2017. EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis. In EACL 2017 - Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Valencia, Spain, April 3-7, 2017. Volume 2, Short Papers, pages 578-585. Available: http://aclweb.org/anthology/E17-2092\n",
    "\n",
    "Sven Buechel and Udo Hahn. 2017. Readers vs. writers vs. texts: Coping with different perspectives of text understanding in emotion annotation. In LAW 2017 - Proceedings of the 11th Linguistic Annotation Workshop @ EACL 2017. Valencia, Spain, April 3, 2017, pages 1-12. Available: https://sigann.github.io/LAW-XI-2017/papers/LAW01.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>Remember what she said in my last letter? \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>If I wasn't working here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.22</td>\n",
       "      <td>Goodwill helps people get off of public assistance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.46</td>\n",
       "      <td>Sherry learned through our Future Works class that she could rise out of the mire of the welfare system and support ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split     V     A     D  \\\n",
       "0  train  3.00  3.00  3.20   \n",
       "1   test  2.80  3.10  2.80   \n",
       "2  train  3.00  3.00  3.00   \n",
       "3  train  3.44  3.00  3.22   \n",
       "4  train  3.55  3.27  3.46   \n",
       "\n",
       "                                                                                                                      text  \n",
       "0                                                                              Remember what she said in my last letter? \"  \n",
       "1                                                                                                If I wasn't working here.  \n",
       "2                                                                                                                      ..\"  \n",
       "3                                                                      Goodwill helps people get off of public assistance.  \n",
       "4  Sherry learned through our Future Works class that she could rise out of the mire of the welfare system and support ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb = pd.read_csv(\"C:\\\\Users\\\\fast\\\\camilo\\\\ST7\\\\EmoBank-master\\\\corpus\\\\emobank.csv\")\n",
    "pd.set_option('max_colwidth', 120)\n",
    "eb = eb.drop(['id'], axis=1)\n",
    "eb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms of Valence, Arousal and Dominance respectively\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Histograms of Valence, Arousal and Dominance respectively')\n",
    "valence, arousal, dominance = eb['V'], eb['A'], eb['D']\n",
    "VAR = [valence, arousal, dominance]\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.hist(VAR[i], bins=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRC VAD lexicon\n",
    "\n",
    "[Lexicon from Saif M. Mohammad](http://www.saifmohammad.com/WebPages/lexicons.html)\n",
    "\n",
    "Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words. Saif M. Mohammad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia, July 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valence</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Dominance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaaaaaah</th>\n",
       "      <td>0.479</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaah</th>\n",
       "      <td>0.520</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aardvark</th>\n",
       "      <td>0.427</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aback</th>\n",
       "      <td>0.385</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacus</th>\n",
       "      <td>0.510</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Valence  Arousal  Dominance\n",
       "Word                                 \n",
       "aaaaaaah    0.479    0.606      0.291\n",
       "aaaah       0.520    0.636      0.282\n",
       "aardvark    0.427    0.490      0.437\n",
       "aback       0.385    0.407      0.288\n",
       "abacus      0.510    0.276      0.485"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAD_words = pd.read_csv(\"NRC-VAD-Lexicon.txt\",sep='\\t',header=0, index_col='Word')\n",
    "VAD_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms of Valence, Arousal and Dominance respectively, word level\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Histograms of Valence, Arousal and Dominance respectively, word level')\n",
    "VAR_w = [VAD_words['Valence'], VAD_words['Arousal'], VAD_words['Dominance']]\n",
    "fig, axes = plt.subplots(1,3,figsize=(16,3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.hist(VAR_w[i], bins=50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try different approaches!\n",
    "Goal is to predict the VAD values of a sentence from its input. Then it would also be great to acquire some model understanding on the way!\n",
    "## Approach 1 : average of word's VAD value\n",
    "An intuitive way of facing this problem could be to just take the mean of the VAD values for each word in the sentence. That way, if we had more negative words (valence) the sentence would naturally be more negative. The parameters to learn in this case would be those of the simple linear relation:\n",
    "$$VADsentence(x_1, x_2, \\dots x_n) = a*(VADword(x_1)+VADword(x_2)+ \\dots +VADword(x_n))/n+b$$\n",
    "Notice that they are all 3 component vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coef(x, y): \n",
    "    # number of observations/points \n",
    "    n = np.size(x)\n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    a = SS_xy / SS_xx \n",
    "    b = m_y - a*m_x \n",
    "  \n",
    "    return(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalizeString(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(word):\n",
    "    #returns vad+300 glove vector\n",
    "    return VAD_words.loc[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_VAD_mean(sentence):\n",
    "    #takes a string (sentence) and returns the average VAD vector\n",
    "    sentence = sentence.lower()\n",
    "    sentence = normalizeString(sentence)\n",
    "    words = sentence.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = [i for i in VAD_words.loc[word][:3]]\n",
    "            vectors.append(vec)\n",
    "        except:\n",
    "            #if word not in the lexicon then it must be neutral\n",
    "            vectors.append([.5, .5, .5])\n",
    "    valence = np.mean([vector[0] for vector in vectors])\n",
    "    arrousal = np.mean([vector[1] for vector in vectors])\n",
    "    dominance = np.mean([vector[2] for vector in vectors])\n",
    "    return valence, arrousal, dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5456666666666666, 0.46677777777777785, 0.4773333333333334)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_VAD_mean('Remember what she said in my last letter? \"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add columns to keep the VAD average values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb[\"V_words\"] = \"\"\n",
    "eb[\"A_words\"] = \"\"\n",
    "eb[\"D_words\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eb)):\n",
    "    eb['V_words'][i], eb['A_words'][i], eb['D_words'][i] = compute_VAD_mean(eb['text'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only consider the training sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = eb[eb.split=='train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating coefficients for valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for Valence: a = 2.839421168278393, b = 1.454510105282507\n"
     ]
    }
   ],
   "source": [
    "a_val, b_val = estimate_coef(train_1['V_words'], train_1['V'])\n",
    "print('Coefficients for Valence: a = '+str(a_val)+', b = '+str(b_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the approach\n",
    "We will now consider the test set, we'll predict the valence of the text and we'll compare it with the actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = eb[eb.split=='test']\n",
    "prediction_1_val = []\n",
    "for i in test_1['V_words']:\n",
    "    prediction_1_val.append(a_val*i + b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 error of approach 1 for valence = 0.2287386269235124\n"
     ]
    }
   ],
   "source": [
    "Error_Val = np.absolute((prediction_1_val - test_1['V'].values)).mean()\n",
    "print('L1 error of approach 1 for valence = '+str(Error_Val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a couple of examples to see if it isn't a generalisation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "When my father died, a year after my mother, my sisters and I cleaned out their apartment.\n",
      "Valence = 2.0, predicted valence = 2.9570371401631563\n",
      "--------------------------------------------------------------------------------------\n",
      "Outdated baby food found on shelves\n",
      "Valence = 1.89, predicted valence = 3.217317413922009\n",
      "--------------------------------------------------------------------------------------\n",
      "So empty it hurt.\n",
      "Valence = 2.0, predicted valence = 2.4483075141799446\n",
      "--------------------------------------------------------------------------------------\n",
      "\"This is a criminal act and it certainly puts things in a different league,\"\n",
      "Valence = 2.0, predicted valence = 2.852316583266413\n"
     ]
    }
   ],
   "source": [
    "for i in [7518, 4949, 7066, 2947]:\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    val_word = test_1['V_words'][i]\n",
    "    val, pred = test_1['V'][i], val_word*a_val + b_val\n",
    "    print(test_1['text'][i])\n",
    "    print('Valence = {}, predicted valence = {}'.format(val,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "And the last lesson -- have fun .\n",
      "Valence = 4.0, predicted valence = 3.2307708618383764\n",
      "--------------------------------------------------------------------------------------\n",
      "Canadian breakthrough offers hope on autism\n",
      "Valence = 3.88, predicted valence = 3.036067696013572\n",
      "--------------------------------------------------------------------------------------\n",
      "We slammed against the doorway and I was laughing too, the pulse close enough to shake the doorframe and set up vibrations in my chest, Rachel in my arms because she’d used me to soften her landing.\n",
      "Valence = 4.1, predicted valence = 2.8976641154777454\n",
      "--------------------------------------------------------------------------------------\n",
      "The proverbial hospitality and warm welcome are still here.\n",
      "Valence = 3.75, predicted valence = 3.188260670633294\n"
     ]
    }
   ],
   "source": [
    "for i in [4455, 4618, 3871, 2613]:\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    val_word = test_1['V_words'][i]\n",
    "    val, pred = test_1['V'][i], val_word*a_val + b_val\n",
    "    print(test_1['text'][i])\n",
    "    print('Valence = {}, predicted valence = {}'.format(val,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for arrousal and dominance we have the following errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 error of approach 1 for valence = 0.186054452901469\n",
      "L1 error of approach 1 for valence = 0.15509522160344397\n"
     ]
    }
   ],
   "source": [
    "a_arr, b_arr = estimate_coef(train_1['A_words'], train_1['A'])\n",
    "a_dom, b_dom = estimate_coef(train_1['D_words'], train_1['D'])\n",
    "prediction_1_arr, prediction_1_dom = [], []\n",
    "for i in test_1['A_words']:\n",
    "    prediction_1_arr.append(a_arr*i + b_arr)\n",
    "for i in test_1['D_words']:\n",
    "    prediction_1_dom.append(a_dom*i + b_dom)\n",
    "Error_Arr = np.absolute((prediction_1_arr - test_1['A'].values)).mean()\n",
    "print('L1 error of approach 1 for arrousal = '+str(Error_Arr))\n",
    "Error_Dom = np.absolute((prediction_1_dom - test_1['D'].values)).mean()\n",
    "print('L1 error of approach 1 for dominance = '+str(Error_Dom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 : Neural network approach\n",
    "We can think of this taks as a function that we have to approximate. Neural networks are a nowadays an effective (and popular) way to approximate functions. Since our input corresponds to a sequence of elements (words in this case), then we'll use a recurrent neural network (NRR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use [PyTorch](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "### Sentences to tensors\n",
    "\n",
    "To represent each word we use the 300 dimensional word vectors from Glove.\n",
    "\n",
    "[Pre-trained word embedding vectors](https://centralesupelec-my.sharepoint.com/personal/camilo_carvajal_student-cs_fr/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fcamilo%5Fcarvajal%5Fstudent%2Dcs%5Ffr%2FDocuments%2FCS%2BR%2FPretrained%20word%20vectors&originalPath=aHR0cHM6Ly9jZW50cmFsZXN1cGVsZWMtbXkuc2hhcmVwb2ludC5jb20vOmY6L2cvcGVyc29uYWwvY2FtaWxvX2NhcnZhamFsX3N0dWRlbnQtY3NfZnIvRXJsMG9adkNSLXRFdnNWbnBnUDNKbXdCQzlKbzQxRDVsQVBJZVBNYWJpUERBQT9ydGltZT1LSzZFd0NEaTEwZw)\n",
    "\n",
    "We load the pickled dataframe\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.082752</td>\n",
       "      <td>0.672040</td>\n",
       "      <td>-0.14987</td>\n",
       "      <td>-0.064983</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>0.402280</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>-0.331100</td>\n",
       "      <td>-0.306910</td>\n",
       "      <td>2.0817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14331</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>-0.18643</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-0.355980</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>-0.050821</td>\n",
       "      <td>-0.191800</td>\n",
       "      <td>-0.378460</td>\n",
       "      <td>-0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>-0.12578</td>\n",
       "      <td>-0.593250</td>\n",
       "      <td>0.125250</td>\n",
       "      <td>0.159750</td>\n",
       "      <td>0.137480</td>\n",
       "      <td>-0.331570</td>\n",
       "      <td>-0.136940</td>\n",
       "      <td>1.7893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.066737</td>\n",
       "      <td>-0.29556</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>-0.281350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>0.138710</td>\n",
       "      <td>-0.360490</td>\n",
       "      <td>-0.03500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.272040</td>\n",
       "      <td>-0.062030</td>\n",
       "      <td>-0.18840</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>-0.138770</td>\n",
       "      <td>0.177080</td>\n",
       "      <td>0.177090</td>\n",
       "      <td>2.5882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42810</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>0.22511</td>\n",
       "      <td>-0.285570</td>\n",
       "      <td>-0.102800</td>\n",
       "      <td>-0.018168</td>\n",
       "      <td>0.114070</td>\n",
       "      <td>0.130150</td>\n",
       "      <td>-0.183170</td>\n",
       "      <td>0.13230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.185670</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>-0.25209</td>\n",
       "      <td>-0.117250</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>0.064908</td>\n",
       "      <td>0.122910</td>\n",
       "      <td>-0.093979</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59396</td>\n",
       "      <td>-0.097729</td>\n",
       "      <td>0.20072</td>\n",
       "      <td>0.170550</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>-0.039709</td>\n",
       "      <td>0.324980</td>\n",
       "      <td>-0.023452</td>\n",
       "      <td>0.123020</td>\n",
       "      <td>0.33120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.319240</td>\n",
       "      <td>0.063160</td>\n",
       "      <td>-0.27858</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>-0.214620</td>\n",
       "      <td>-0.104950</td>\n",
       "      <td>0.154950</td>\n",
       "      <td>-0.033530</td>\n",
       "      <td>2.4834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12977</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.18888</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.106450</td>\n",
       "      <td>-0.258100</td>\n",
       "      <td>-0.044629</td>\n",
       "      <td>0.082745</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.25045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2        3         4         5         6         7    \\\n",
       "0                                                                          \n",
       ",   -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n",
       ".    0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \n",
       "the  0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \n",
       "and -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \n",
       "to   0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n",
       "\n",
       "          8         9       10   ...      291       292      293       294  \\\n",
       "0                                ...                                         \n",
       ",   -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n",
       ".   -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \n",
       "the  0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \n",
       "and -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \n",
       "to   0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n",
       "\n",
       "          295       296       297       298       299      300  \n",
       "0                                                               \n",
       ",   -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n",
       ".   -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \n",
       "the -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \n",
       "and -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \n",
       "to  -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = r'C:\\Users\\fast\\camilo\\Research_Project\\Databases\\glove.840B.300d.pkl'\n",
    "#your path\n",
    "glove_vecs = pd.read_pickle(txt)\n",
    "glove_vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a sentence we join them into a 2D matrix. However the dimensions are given by:``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "\n",
    "def WordToTensor(word):\n",
    "    vec_word = [n for n in glove_vecs.loc[word]]\n",
    "    tensor = torch.FloatTensor(vec_word)\n",
    "    return tensor\n",
    "\n",
    "def SentenceToTensor(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = normalizeString(sentence)\n",
    "    words = sentence.split()\n",
    "    len_sentence = len(words)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec_word = glove_vecs.loc[word]\n",
    "        except:\n",
    "            len_sentence += -1\n",
    "    if len_sentence==0:\n",
    "        return torch.zeros(1, 1, 300)\n",
    "    tensor = torch.zeros(len_sentence, 1, 300)\n",
    "    for index, word in enumerate(words):\n",
    "        tensor[index][0][:] = WordToTensor(word)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 300])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hola = SentenceToTensor('what is love?')\n",
    "hola.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 600\n",
    "n_categories = 3\n",
    "rnn = RNN(300, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2202, -0.0097,  0.0742]], grad_fn=<AddmmBackward>) tensor([[-1.0166e-02, -1.0015e-01, -2.2861e-02,  2.6831e-01,  1.3063e-01,\n",
      "         -9.4053e-04,  2.4239e-01, -2.5919e-01, -6.1307e-02, -2.2866e-01,\n",
      "          2.0585e-01, -2.8933e-02,  9.9936e-03, -2.9961e-02,  4.8870e-02,\n",
      "         -7.8813e-03,  1.6611e-01,  5.0352e-02,  6.8945e-02,  1.2631e-01,\n",
      "          1.6805e-01, -1.0975e-01,  3.0740e-02, -2.5084e-02,  8.8976e-02,\n",
      "          9.3081e-02,  1.1318e-01,  1.9734e-01,  6.3956e-02, -2.3526e-01,\n",
      "         -1.5554e-02,  4.0989e-02,  9.0401e-02, -4.8504e-02, -8.3495e-03,\n",
      "         -8.0738e-02,  7.6810e-02, -4.5258e-02, -1.1778e-01,  9.1672e-03,\n",
      "          1.8153e-02, -2.8526e-02, -1.5026e-01,  7.9682e-02,  2.3485e-01,\n",
      "          4.4604e-02,  5.5264e-02, -7.7435e-02,  2.3331e-01, -1.9547e-01,\n",
      "         -1.7859e-01, -7.1762e-02, -4.2446e-02,  3.0213e-01,  1.9971e-01,\n",
      "         -1.5021e-01,  4.6005e-02,  5.3127e-02,  8.8054e-02,  4.3298e-02,\n",
      "          6.3428e-02,  1.8832e-01, -7.0685e-02, -9.5559e-02,  1.2916e-01,\n",
      "         -8.6688e-02,  8.5491e-02,  1.6455e-01,  1.5662e-01,  4.7313e-03,\n",
      "         -5.9830e-02, -8.7442e-03,  1.5968e-02,  5.7842e-02, -6.7542e-03,\n",
      "         -5.1931e-02, -5.0931e-02, -1.8344e-01, -2.7398e-02, -2.4714e-01,\n",
      "         -4.4348e-02, -3.4338e-02, -1.0841e-01, -1.1966e-01,  6.2861e-02,\n",
      "         -9.2111e-02, -2.3059e-01, -7.2872e-02,  2.0802e-01, -2.1094e-01,\n",
      "          2.7180e-02,  1.9370e-01, -1.2806e-01,  6.3444e-02,  7.7237e-02,\n",
      "          6.1039e-02,  3.4714e-02,  2.0923e-02,  9.5067e-02,  1.9910e-02,\n",
      "          1.7423e-02,  3.4521e-01,  1.0854e-01, -7.6986e-02, -7.0973e-02,\n",
      "          3.3336e-01, -2.0195e-01, -8.1941e-02,  1.2504e-01,  1.5830e-01,\n",
      "          3.4062e-03, -1.6107e-01,  5.1265e-02,  3.6664e-02,  1.7459e-02,\n",
      "          1.8078e-01, -2.6782e-02,  1.8428e-01, -5.7449e-02, -6.5006e-02,\n",
      "         -6.3547e-02, -6.7936e-03, -2.3589e-01, -1.4623e-02,  6.8141e-02,\n",
      "         -9.4631e-03,  1.5997e-01, -3.8732e-04, -1.0744e-01,  3.2095e-02,\n",
      "          2.3282e-01,  1.3725e-01, -1.5509e-01, -1.6309e-01, -5.1914e-03,\n",
      "         -1.3481e-01, -1.6835e-01,  5.1068e-02,  8.2066e-02,  4.1135e-03,\n",
      "          1.0719e-01,  4.2256e-02, -1.2170e-01, -1.3753e-01, -1.2947e-01,\n",
      "         -7.2613e-02, -2.4455e-03, -2.2924e-02, -1.7431e-01, -2.1158e-02,\n",
      "         -3.7057e-01, -2.8957e-01, -3.3866e-02, -9.3405e-02, -7.6178e-02,\n",
      "          8.5084e-02, -2.4991e-01, -6.1783e-02, -1.2165e-01,  1.9999e-01,\n",
      "         -1.3235e-01, -1.2272e-01, -1.3006e-01, -6.2449e-02, -1.7371e-01,\n",
      "          1.8226e-01, -2.2638e-01,  1.1195e-01,  3.4838e-01,  1.3779e-01,\n",
      "          2.8145e-01, -1.6189e-02,  1.7679e-01,  2.1416e-02,  1.2122e-01,\n",
      "         -4.9659e-02,  1.2132e-01,  2.5687e-01,  7.6797e-02, -6.0208e-02,\n",
      "         -1.8107e-01, -4.8691e-02, -1.6702e-01,  1.1730e-01,  2.1987e-01,\n",
      "          1.6164e-01,  1.1504e-01,  1.5225e-01, -1.3060e-01,  4.4226e-02,\n",
      "          2.2530e-01, -1.8701e-01, -1.6247e-01,  4.6269e-02, -4.3886e-02,\n",
      "          9.1180e-02, -9.9530e-02, -9.8774e-02, -2.2347e-01,  9.2672e-02,\n",
      "         -1.8630e-01, -2.0452e-01, -1.0135e-01,  3.3166e-02, -1.6178e-01,\n",
      "          2.4838e-01, -3.2815e-02,  8.2750e-02, -1.7280e-01,  4.4091e-02,\n",
      "         -2.1735e-01,  2.7295e-01, -1.4378e-02, -2.3915e-01, -1.0634e-03,\n",
      "         -4.1960e-02, -1.0391e-01, -1.6365e-01,  1.0556e-01, -4.1784e-02,\n",
      "          4.4481e-02, -8.9942e-02, -1.7496e-01, -4.2737e-03, -2.3066e-01,\n",
      "          9.8435e-02, -1.4385e-01, -3.7579e-02, -2.2054e-03,  6.5080e-02,\n",
      "         -1.5796e-01,  3.0816e-01,  6.4553e-02, -3.4604e-02, -6.1271e-02,\n",
      "         -9.7201e-02, -9.0156e-03, -4.2543e-02, -8.7286e-02,  8.9545e-02,\n",
      "         -1.8287e-01, -1.1386e-01,  7.5393e-02,  1.9774e-02, -9.1360e-02,\n",
      "         -6.6386e-03,  9.8203e-03, -1.5352e-01, -9.4242e-02, -6.1803e-02,\n",
      "          1.7181e-01,  3.5153e-02, -7.8581e-02, -5.7109e-02, -2.6307e-01,\n",
      "          1.8154e-01,  2.1455e-01,  1.3514e-01,  8.1496e-02,  1.5795e-01,\n",
      "          2.8018e-02, -6.5479e-02,  3.1153e-02,  5.0667e-02, -6.6734e-03,\n",
      "          6.1032e-02,  5.3347e-02,  1.8633e-01,  7.3550e-02, -5.9990e-02,\n",
      "          7.8644e-02, -1.1438e-01, -1.7900e-01,  6.6757e-02,  1.3019e-02,\n",
      "         -1.5782e-01, -8.1608e-02,  7.7931e-02,  2.5492e-01,  1.5674e-01,\n",
      "          3.7954e-02,  3.8598e-02,  1.9615e-01,  9.8399e-02, -6.2353e-02,\n",
      "          2.4331e-01, -6.1558e-02, -5.8234e-02, -1.5397e-02, -2.1045e-01,\n",
      "         -9.6323e-02,  1.6572e-01,  2.6592e-01, -1.6850e-01,  7.1209e-02,\n",
      "         -1.4805e-01, -7.6246e-02, -4.4193e-02,  5.9288e-02,  9.7802e-02,\n",
      "         -2.0079e-01,  4.0117e-02,  7.2380e-02, -1.3299e-01, -4.4280e-02,\n",
      "          8.5104e-02,  2.3501e-01, -2.6269e-02, -1.0357e-01,  6.7753e-02,\n",
      "          3.1163e-02,  1.6932e-01, -4.2560e-02, -1.3540e-01, -1.5831e-01,\n",
      "         -1.3378e-01, -5.3416e-02,  1.7284e-01,  2.3803e-01,  2.1835e-02,\n",
      "         -7.2244e-02, -8.0781e-03,  2.9215e-02, -6.9165e-02,  4.4332e-02,\n",
      "         -3.2369e-03, -1.3646e-01,  1.2577e-01, -4.9887e-02, -4.3821e-02,\n",
      "          2.7358e-02, -3.9995e-02,  2.8848e-02, -2.0806e-01,  1.1973e-02,\n",
      "          2.2997e-02, -6.9591e-02,  4.8345e-02,  5.1621e-02, -1.1856e-01,\n",
      "          3.4725e-02,  2.3378e-01,  1.4660e-01,  2.2666e-02, -1.5973e-01,\n",
      "         -2.7200e-01, -1.2559e-02, -1.3108e-01, -3.1637e-02, -1.5107e-01,\n",
      "         -2.8834e-02, -8.1210e-02, -3.4825e-03, -4.9784e-02,  1.5316e-01,\n",
      "          3.4608e-02, -3.4163e-02, -8.1778e-03, -4.3168e-02, -1.7625e-01,\n",
      "          4.1569e-02, -1.5355e-01,  1.9600e-01, -2.9944e-01, -5.0397e-02,\n",
      "          1.2381e-01,  2.0119e-03,  2.0554e-01, -1.3697e-01, -3.0515e-03,\n",
      "          1.7351e-01, -2.0544e-01, -3.6214e-03,  2.7058e-02, -9.7531e-02,\n",
      "         -7.1343e-02,  7.6265e-02, -3.8349e-02,  1.0646e-01,  4.2066e-02,\n",
      "          8.0768e-03, -2.8038e-01,  9.5948e-02,  6.4948e-02, -2.2322e-01,\n",
      "         -6.8843e-02, -1.4388e-01,  9.5338e-02,  9.8430e-02,  6.9502e-02,\n",
      "          3.4843e-02, -1.6231e-01, -4.4119e-04, -6.2656e-02, -1.2505e-01,\n",
      "         -1.8461e-01,  4.3598e-02,  6.4387e-02,  1.0243e-02,  1.0491e-01,\n",
      "         -2.2256e-02, -4.1269e-02,  1.8438e-01,  2.9779e-01, -1.4019e-01,\n",
      "          1.0766e-01,  1.0043e-01, -2.3725e-01,  2.4503e-01, -4.6204e-02,\n",
      "         -1.4506e-02,  5.2102e-02,  1.1984e-01, -2.3371e-02,  1.9885e-01,\n",
      "         -1.2355e-01, -5.0225e-02, -1.7735e-02, -5.0727e-02, -2.1707e-01,\n",
      "         -1.8843e-01, -5.9140e-02, -1.0143e-01,  1.2801e-01,  1.0460e-01,\n",
      "         -5.4143e-02,  2.5847e-01, -1.5845e-01,  5.4146e-02, -3.5953e-03,\n",
      "          5.4962e-02,  1.3103e-01, -1.4394e-01, -2.4478e-02,  1.0936e-01,\n",
      "          9.0411e-03,  5.6945e-02,  5.5044e-02, -1.2222e-01,  5.6947e-02,\n",
      "          7.8947e-02,  9.1160e-02, -8.3403e-02, -1.8557e-01, -4.7390e-02,\n",
      "          4.5238e-02,  5.0634e-02, -5.1031e-03,  5.4442e-03, -7.9674e-02,\n",
      "         -3.1382e-02, -1.8444e-01, -2.3141e-02,  1.2651e-02, -1.2263e-01,\n",
      "          1.6501e-01, -1.0092e-01,  7.7160e-02,  3.2301e-02,  9.8039e-02,\n",
      "         -1.2290e-01,  1.9674e-01, -1.8893e-01, -1.1604e-01,  4.6166e-02,\n",
      "          1.6554e-01,  4.7396e-02, -8.8590e-03, -1.5022e-01, -2.1608e-01,\n",
      "          3.3755e-02, -2.7358e-02,  7.3665e-02,  2.8844e-01, -9.2078e-02,\n",
      "          6.0634e-02,  2.6796e-01, -3.9111e-02,  2.7483e-04,  1.2100e-01,\n",
      "          1.1016e-01, -1.5260e-01, -1.5488e-01, -1.0034e-01, -3.0406e-02,\n",
      "         -1.0230e-01, -9.5013e-02, -2.7954e-02, -1.6961e-02, -4.5782e-03,\n",
      "          2.2034e-01,  2.5526e-01, -4.7708e-02, -3.9042e-02,  8.1479e-02,\n",
      "         -1.0081e-01, -5.9979e-02, -3.2685e-02,  1.8460e-01, -4.5720e-02,\n",
      "          2.6527e-02,  8.3884e-03, -4.4760e-01,  9.5647e-02, -2.0803e-02,\n",
      "         -1.4316e-01,  4.0282e-02,  9.5955e-02,  3.3098e-02, -8.0749e-02,\n",
      "          1.0049e-01, -4.4297e-02, -3.8741e-02, -1.7944e-01, -1.2230e-01,\n",
      "          2.6822e-06,  1.9051e-01,  2.8040e-02,  2.2363e-01,  1.8734e-01,\n",
      "          3.0220e-01, -2.1853e-01, -1.0129e-01,  3.0838e-02,  1.2027e-01,\n",
      "         -5.2942e-02, -1.1010e-01, -1.6855e-01,  2.6601e-02,  1.8803e-01,\n",
      "          1.0726e-01, -6.4906e-02, -1.7005e-01, -1.0309e-01, -3.4517e-02,\n",
      "         -2.2770e-02, -1.2781e-01,  2.0519e-01,  8.9788e-03, -6.3115e-02,\n",
      "          1.5428e-02,  9.3958e-02, -8.7865e-02,  1.2470e-01, -3.3153e-02,\n",
      "         -2.1883e-01, -1.1260e-01,  2.8461e-01,  1.2263e-01,  1.3088e-01,\n",
      "         -4.0828e-02,  4.5985e-02,  2.0915e-01, -1.1046e-02,  2.3003e-01,\n",
      "         -2.6069e-01,  1.1136e-01, -2.2383e-02,  1.1590e-01, -2.7937e-02,\n",
      "          3.9190e-02,  1.4613e-01, -9.6391e-02,  6.1430e-02, -1.5564e-01,\n",
      "          8.7996e-02, -8.1276e-03,  2.0256e-01, -7.9390e-02,  6.4283e-02,\n",
      "         -2.2001e-03, -2.7499e-01,  8.6605e-02, -1.3037e-01,  5.1595e-02,\n",
      "         -9.6388e-02, -6.3183e-02, -2.7782e-01, -1.3955e-02, -2.6826e-02,\n",
      "          9.2247e-03,  1.8141e-01, -9.3616e-02,  1.3731e-01,  2.4611e-02,\n",
      "         -6.4305e-02, -2.1589e-01,  1.9839e-02,  2.9971e-01, -2.5760e-01,\n",
      "         -5.9293e-02, -2.0270e-01, -2.3837e-01,  7.5091e-02,  5.2936e-02,\n",
      "         -5.7503e-02,  7.3338e-02,  2.2002e-02,  1.5702e-02,  7.9950e-03]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = SentenceToTensor('baby dont hurt me')\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output, next_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training and test set, defining auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>V</th>\n",
       "      <th>A</th>\n",
       "      <th>D</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>Remember what she said in my last letter? \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.22</td>\n",
       "      <td>Goodwill helps people get off of public assistance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.46</td>\n",
       "      <td>Sherry learned through our Future Works class that she could rise out of the mire of the welfare system and support ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.80</td>\n",
       "      <td>Coming to Goodwill was the first step toward my becoming totally independent.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split     V     A     D  \\\n",
       "0  train  3.00  3.00  3.20   \n",
       "2  train  3.00  3.00  3.00   \n",
       "3  train  3.44  3.00  3.22   \n",
       "4  train  3.55  3.27  3.46   \n",
       "5  train  3.60  3.30  3.80   \n",
       "\n",
       "                                                                                                                      text  \n",
       "0                                                                              Remember what she said in my last letter? \"  \n",
       "2                                                                                                                      ..\"  \n",
       "3                                                                      Goodwill helps people get off of public assistance.  \n",
       "4  Sherry learned through our Future Works class that she could rise out of the mire of the welfare system and support ...  \n",
       "5                                            Coming to Goodwill was the first step toward my becoming totally independent.  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2 = test_1.drop(['V_words', 'A_words', 'D_words'], axis=1)\n",
    "train_2 = train_1.drop(['V_words', 'A_words', 'D_words'], axis=1)\n",
    "train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(dataFrame):\n",
    "    return dataFrame.loc[random.choice(dataFrame.index)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    row = randomChoice(train_2)\n",
    "    try:\n",
    "        VAD = [row.V, row.A, row.D]\n",
    "        tensor_VAD = torch.FloatTensor([[VAD]])\n",
    "        sentence = row.text\n",
    "        tensor_sentence = SentenceToTensor(sentence)\n",
    "    except:\n",
    "        sentence, VAD, tensor_sentence, tensor_VAD = randomTrainingExample()\n",
    "    return sentence, VAD, tensor_sentence, tensor_VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTestExample():\n",
    "    row1, row2, row3 = randomChoice(test_2), randomChoice(test_2), randomChoice(test_2)\n",
    "    row4, row5 = randomChoice(test_2), randomChoice(test_2)\n",
    "    tensor_sentence_tests, VAD_tests = [], []\n",
    "    for row in [row1, row2, row3, row4, row5]:\n",
    "        try:\n",
    "            VAD = [row.V, row.A, row.D]\n",
    "            tensor_VAD = torch.FloatTensor([[VAD]])\n",
    "            sentence = row.text\n",
    "            tensor_sentence = SentenceToTensor(sentence)\n",
    "            tensor_sentence_tests.append(tensor_sentence)\n",
    "            VAD_tests.append(tensor_VAD)\n",
    "        except:\n",
    "            nothing = True\n",
    "    return tensor_sentence_tests, VAD_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an evaluation function to constantly test the network while training, but we will use it later to do the overall evaluation and to predict valence for new examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(sentence_tensor.size()[0]):\n",
    "        output, hidden = rnn(sentence_tensor[i], hidden)\n",
    "\n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will evaluate constantly how are function is performing on 5 random test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_while_training():\n",
    "    tensor_sentence_tests, VAD_tests = randomTestExample()\n",
    "    sum_error = 0\n",
    "    #test_2_val, test_2_arr, test_2_dom = [], [], []\n",
    "    for i in range(len(tensor_sentence_tests)):\n",
    "        #pred_val, pred_arr, pred_dom = evaluate(s_t)[0], evaluate(s_t)[1], evaluate(s_t)[2]\n",
    "        output = evaluate(tensor_sentence_tests[i])\n",
    "        sum_error += criterion(output, VAD_tests[i])\n",
    "    #Error_Val = ((test_2_val - VAD_tests)**2).mean()\n",
    "    #Error_Arr = ((test_2_arr - VAD_.values)**2).mean()\n",
    "    #Error_Dom = ((test_2_dom - test_2['D'].values).mean()\n",
    "    return sum_error/len(tensor_sentence_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Never gonna give you ...'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just a function to save room on the examples to show while training\n",
    "def portionSentence(string,limit=20):\n",
    "    if len(string)<limit:\n",
    "        return string\n",
    "    else:\n",
    "        new_str = string[:limit]+' ...'\n",
    "        return new_str\n",
    "portionSentence('Never gonna give you up, Never gonna let you down')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "Or loading pre-trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(VAD_tensor, Sentence_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(Sentence_tensor.size()[0]):\n",
    "        output, hidden = rnn(Sentence_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, VAD_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0% (1m 16s) 0.0841 Sure. / tensor([2.5340, 2.4967, 2.6225]) [2.9, 2.7, 2.9]\n",
      "200 1% (2m 49s) 0.1945 Poverty is frequentl ... / tensor([3.1926, 2.9821, 3.1789]) [2.5, 3.25, 3.0]\n",
      "300 1% (4m 21s) 0.0521 First of America, wh ... / tensor([3.1114, 2.8400, 3.1627]) [3.2, 3.2, 3.3]\n",
      "400 2% (6m 1s) 0.1480 I was noodling on Va ... / tensor([2.8969, 2.6542, 2.9484]) [3.1, 3.1, 3.4]\n",
      "500 2% (7m 31s) 0.0920 The effects of Spani ... / tensor([3.1811, 3.2759, 3.1321]) [2.86, 2.86, 3.14]\n",
      "600 3% (9m 4s) 0.2342 The vampire fell for ... / tensor([3.0307, 3.1321, 3.0867]) [2.38, 3.38, 2.62]\n",
      "700 3% (10m 30s) 0.0158 I wondered why, if h ... / tensor([2.6677, 2.9081, 2.7592]) [2.62, 3.12, 2.75]\n",
      "800 4% (12m 3s) 0.1545 Enough. / tensor([2.5722, 2.7122, 2.7082]) [2.78, 3.11, 3.22]\n",
      "900 4% (13m 36s) 0.1572 House passes anti-te ... / tensor([2.4436, 2.7338, 2.5333]) [3.0, 2.9, 2.9]\n",
      "1000 5% (15m 13s) 0.0100 Portugal’s very prec ... / tensor([3.0874, 3.1324, 3.0705]) [3.0, 3.0, 3.0]\n",
      "1100 5% (16m 46s) 0.1234 Jade. / tensor([2.6968, 2.5851, 2.6827]) [3.0, 2.88, 3.12]\n",
      "1200 6% (18m 19s) 0.0259 “That’s a question y ... / tensor([2.6609, 2.8781, 2.9949]) [2.8, 3.1, 2.9]\n",
      "1300 6% (19m 41s) 0.0469 “always kind of fant ... / tensor([3.1719, 3.1353, 3.3329]) [3.3, 3.4, 3.1]\n",
      "1400 7% (21m 10s) 0.0371 Those included costs ... / tensor([2.9191, 3.1169, 3.0912]) [2.78, 2.89, 2.89]\n",
      "1500 7% (22m 53s) 0.0287 Karnes carries the i ... / tensor([2.9269, 3.1726, 3.1712]) [2.7, 3.1, 3.0]\n",
      "1600 8% (24m 33s) 0.1516 Easterling said he w ... / tensor([2.6540, 2.8776, 2.9214]) [3.1, 3.3, 3.2]\n",
      "1700 8% (25m 54s) 0.0175 Seeing as since the  ... / tensor([3.1249, 3.1170, 3.2717]) [3.09, 2.91, 3.18]\n",
      "1800 9% (27m 16s) 0.0389 He’s sitting right t ... / tensor([3.1259, 3.1723, 3.1044]) [3.0, 3.43, 3.29]\n",
      "1900 9% (28m 46s) 0.1459 Madonna's new baby's ... / tensor([3.2901, 3.3600, 3.3526]) [2.7, 3.2, 3.1]\n",
      "2000 10% (30m 26s) 0.0212 Won't you help make  ... / tensor([2.8842, 3.0862, 3.0339]) [3.0, 3.3, 3.1]\n",
      "2100 10% (32m 3s) 0.0046 It's chlorophyll gra ... / tensor([2.7499, 2.8838, 2.7946]) [2.8, 2.9, 2.9]\n",
      "2200 11% (33m 40s) 0.1346 The end result of su ... / tensor([2.9033, 3.1379, 3.2132]) [2.38, 3.0, 2.88]\n",
      "2300 11% (35m 7s) 0.1168 In her hands was a b ... / tensor([2.9127, 2.6529, 2.7218]) [2.92, 2.92, 3.25]\n",
      "2400 12% (36m 33s) 0.0065 Now the former “Nath ... / tensor([2.9615, 2.9077, 3.0121]) [3.0, 3.0, 3.11]\n",
      "2500 12% (38m 6s) 0.0691 I had to read it twi ... / tensor([2.7704, 2.9365, 3.0978]) [3.0, 3.12, 2.75]\n",
      "2600 13% (39m 45s) 0.0108 These non colors all ... / tensor([3.2650, 3.0350, 3.1000]) [3.2, 2.9, 3.0]\n",
      "2700 13% (41m 19s) 0.0686 Karnes knew the 509t ... / tensor([2.9225, 3.1588, 3.1989]) [2.73, 3.0, 2.82]\n",
      "2800 14% (42m 42s) 0.1175 Her voice had grown  ... / tensor([2.6880, 3.0442, 3.0321]) [2.22, 2.78, 2.78]\n",
      "2900 14% (44m 7s) 0.0672 Malaysia launches to ... / tensor([3.3884, 2.9886, 3.0585]) [3.5, 3.4, 3.2]\n",
      "3000 15% (45m 33s) 0.1268 Friendly fire pilot  ... / tensor([2.5116, 2.7603, 2.8372]) [3.0, 3.1, 3.0]\n",
      "3100 15% (47m 5s) 0.2834 In fact, the USDA ha ... / tensor([2.9954, 3.1792, 3.0658]) [2.2, 3.2, 2.6]\n",
      "3200 16% (48m 39s) 0.7916 EXAMPLE 2: \"Terroris ... / tensor([3.3427, 3.4442, 3.5387]) [1.9, 3.5, 3.0]\n",
      "3300 16% (50m 7s) 0.1240 Make sure it's clear ... / tensor([3.0416, 3.0865, 2.9197]) [3.4, 3.2, 3.4]\n",
      "3400 17% (51m 36s) 0.1091 White is seen as rep ... / tensor([3.0969, 3.0167, 3.1232]) [3.2, 2.5, 2.9]\n",
      "3500 17% (53m 10s) 0.0188 Why do you ask?\" / tensor([2.9277, 3.0407, 3.0634]) [3.0, 3.14, 2.86]\n",
      "3600 18% (54m 35s) 0.0127 Lowering West Street ... / tensor([3.0132, 3.0709, 3.0489]) [3.2, 3.1, 3.0]\n",
      "3700 18% (56m 0s) 0.1365 When he saw Roy in t ... / tensor([2.6541, 2.9453, 2.9649]) [3.25, 3.12, 3.12]\n",
      "3800 19% (57m 34s) 0.1038 “He looks healthy,”  ... / tensor([2.4503, 3.3110, 3.1650]) [3.0, 3.4, 3.2]\n",
      "3900 19% (59m 9s) 0.0229 Closings and cancell ... / tensor([2.3570, 2.8991, 2.8014]) [2.6, 2.9, 2.9]\n",
      "4000 20% (60m 41s) 0.0154 Iran and Nicaraguan  ... / tensor([3.2026, 2.9752, 3.0289]) [3.1, 2.8, 3.1]\n",
      "4100 20% (62m 22s) 0.0065 Both were asked how  ... / tensor([2.9343, 2.9109, 2.9156]) [3.0, 3.0, 3.0]\n",
      "4200 21% (63m 57s) 0.0878 “Who’s the freak?” / tensor([2.6610, 3.2400, 3.0833]) [2.22, 3.33, 3.33]\n",
      "4300 21% (65m 27s) 0.1982 His hair was slightl ... / tensor([2.7042, 3.1491, 2.9562]) [2.3, 2.8, 2.4]\n",
      "4400 22% (67m 0s) 0.1092 Nathan promised to s ... / tensor([2.8880, 3.2096, 3.0419]) [3.36, 3.27, 3.36]\n",
      "4500 22% (68m 32s) 0.0409 When lay theories we ... / tensor([2.6781, 2.9998, 3.0949]) [3.0, 2.9, 3.0]\n",
      "4600 23% (70m 6s) 0.0493 Sincerely, A Mark Gi ... / tensor([2.8417, 2.8721, 2.8876]) [3.0, 2.57, 2.71]\n",
      "4700 23% (71m 41s) 0.0124 I am ready for the m ... / tensor([2.9074, 3.0704, 3.0321]) [2.89, 3.11, 3.22]\n",
      "4800 24% (73m 14s) 0.0691 Sincerely, A Mark Gi ... / tensor([3.0487, 2.8821, 3.0380]) [3.0, 2.57, 2.71]\n",
      "4900 24% (74m 44s) 0.0300 A confessional too b ... / tensor([3.0147, 2.9765, 3.0149]) [3.25, 3.13, 3.12]\n",
      "5000 25% (76m 10s) 0.6444 They gazed at each o ... / tensor([2.6853, 3.0874, 3.1251]) [3.8, 3.9, 3.3]\n",
      "5100 25% (77m 30s) 0.1526 Palestinian factions ... / tensor([2.7101, 2.7306, 2.6886]) [3.1, 3.1, 3.1]\n",
      "5200 26% (78m 55s) 0.1014 Really? / tensor([2.6654, 2.9012, 2.7999]) [2.9, 3.3, 3.1]\n",
      "5300 26% (80m 23s) 0.0509 Brazil Air Force Cit ... / tensor([2.3497, 3.4752, 3.0679]) [2.38, 3.25, 2.75]\n",
      "5400 27% (81m 58s) 0.0823 The Anti-Terrorist - ... / tensor([3.1681, 3.4143, 3.3763]) [2.9, 3.1, 3.1]\n",
      "5500 27% (83m 23s) 0.0216 In fact, Moore menti ... / tensor([2.8148, 2.9335, 2.9388]) [3.0, 3.0, 3.1]\n",
      "5600 28% (84m 50s) 0.0216 Gently Weeps' lyrics ... / tensor([2.9771, 2.8950, 2.9131]) [3.1, 3.1, 3.0]\n",
      "5700 28% (86m 19s) 0.0547 Glancing down the st ... / tensor([2.8379, 3.1497, 3.0177]) [2.5, 3.1, 2.8]\n",
      "5800 28% (87m 52s) 0.1908 \"I don't recall ever ... / tensor([3.1371, 3.3037, 3.3736]) [2.6, 2.8, 3.2]\n",
      "5900 29% (89m 16s) 0.3093 Hatton hands back IB ... / tensor([3.4446, 3.4671, 3.3316]) [2.9, 2.8, 2.9]\n",
      "6000 30% (90m 38s) 0.0163 This enormous, moder ... / tensor([3.0337, 3.1040, 3.0778]) [3.0, 2.9, 3.0]\n",
      "6100 30% (92m 11s) 0.1241 PRESCRIPTION PRIVILE ... / tensor([3.1737, 3.2560, 3.2417]) [2.62, 3.0, 3.25]\n",
      "6200 31% (93m 52s) 0.0048 North Korea aid dema ... / tensor([2.8801, 3.0538, 2.9631]) [2.78, 3.0, 3.0]\n",
      "6300 31% (95m 21s) 0.0708 Putin vows to tackle ... / tensor([3.0393, 3.2214, 3.2225]) [2.86, 2.86, 3.0]\n",
      "6400 32% (96m 52s) 0.0191 BIRMINGHAM, Ala. --  ... / tensor([2.9515, 3.0923, 3.2286]) [3.0, 2.89, 3.11]\n",
      "6500 32% (98m 26s) 0.2873 Many of the children ... / tensor([2.9881, 2.9817, 2.9407]) [2.12, 3.25, 2.75]\n",
      "6600 33% (100m 11s) 0.0125 The McAlpine family, ... / tensor([3.0614, 2.9092, 3.1730]) [3.22, 3.0, 3.11]\n",
      "6700 33% (101m 37s) 0.2540 It never failed to a ... / tensor([3.1605, 3.2795, 3.1426]) [4.0, 3.25, 3.38]\n",
      "6800 34% (103m 7s) 0.0194 Adult Day Care for s ... / tensor([2.9800, 2.8755, 2.8943]) [3.0, 3.0, 3.1]\n",
      "6900 34% (104m 42s) 0.1301 “I've been having he ... / tensor([2.8278, 3.2588, 3.0110]) [2.27, 3.27, 2.73]\n",
      "7000 35% (106m 14s) 0.0283 As it has been an ex ... / tensor([3.3178, 3.2541, 3.3874]) [3.3, 3.3, 3.1]\n",
      "7100 35% (107m 39s) 0.0305 There will be two in ... / tensor([3.0097, 3.3018, 3.1833]) [3.0, 3.0, 3.2]\n",
      "7200 36% (109m 10s) 0.0450 The stone was as war ... / tensor([2.8514, 2.9416, 2.8913]) [3.2, 2.9, 3.0]\n",
      "7300 36% (110m 40s) 0.1147 ” whose feelings of  ... / tensor([3.5041, 3.2987, 3.4250]) [3.0, 3.1, 3.2]\n",
      "7400 37% (112m 12s) 0.0859 The peninsula opposi ... / tensor([3.3798, 2.9967, 3.0332]) [3.0, 3.0, 3.37]\n",
      "7500 37% (113m 36s) 0.2279 Put it in writing, d ... / tensor([2.9349, 3.1079, 3.0026]) [2.6, 3.4, 3.7]\n",
      "7600 38% (115m 2s) 0.0724 But I could handle a ... / tensor([2.7697, 3.0325, 2.9780]) [3.1, 3.1, 3.3]\n",
      "7700 38% (116m 19s) 0.1626 Name Address City, S ... / tensor([2.5856, 2.3884, 2.6171]) [3.0, 2.8, 3.0]\n",
      "7800 39% (117m 47s) 0.2851 Indonesian with bird ... / tensor([2.0832, 3.0371, 2.3337]) [2.0, 3.5, 3.13]\n",
      "7900 39% (119m 21s) 0.0224 Iraqi death toll exc ... / tensor([2.7804, 2.9329, 3.0817]) [2.6, 3.1, 3.0]\n",
      "8000 40% (120m 51s) 0.0041 I'd read Watchmen a  ... / tensor([3.0019, 3.0534, 3.1220]) [3.1, 3.1, 3.1]\n",
      "8100 40% (122m 21s) 0.0178 But he has had to ba ... / tensor([2.8792, 2.9162, 2.8804]) [2.89, 2.89, 3.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200 41% (124m 0s) 0.0461 If I am capable of l ... / tensor([3.2685, 3.1517, 3.1835]) [3.56, 3.33, 3.33]\n",
      "8300 41% (125m 27s) 0.0023 Presumably after bei ... / tensor([2.9641, 2.8301, 2.9435]) [2.9, 2.8, 2.9]\n",
      "8400 42% (127m 2s) 0.1206 She gave me a grumpy ... / tensor([2.8244, 3.0764, 3.0406]) [2.25, 3.25, 3.0]\n",
      "8500 42% (128m 39s) 0.0402 It tells viewers: Ah ... / tensor([2.9617, 3.0946, 3.1223]) [3.3, 3.1, 3.2]\n",
      "8600 43% (130m 15s) 0.0481 And proof that Bush  ... / tensor([2.8336, 2.9033, 3.0400]) [2.6, 3.2, 3.0]\n",
      "8700 43% (131m 42s) 0.1458 When I was done ment ... / tensor([2.9926, 2.9340, 2.9773]) [2.7, 3.5, 2.8]\n",
      "8800 44% (133m 26s) 0.1126 Fermentation: The Da ... / tensor([3.3433, 3.0568, 3.3045]) [3.0, 2.7, 3.0]\n",
      "8900 44% (135m 10s) 0.0129 In his book The Symp ... / tensor([2.9815, 3.0551, 3.1189]) [3.0, 2.9, 3.0]\n",
      "9000 45% (136m 41s) 0.0119 Kun Iam Tong, off Av ... / tensor([3.4063, 3.1795, 3.1709]) [3.4, 3.1, 3.0]\n",
      "9100 45% (138m 11s) 0.0209 One search does not  ... / tensor([2.8985, 2.8797, 3.1018]) [2.67, 2.89, 3.0]\n",
      "9200 46% (139m 39s) 0.0307 This includes a volu ... / tensor([3.1816, 3.0040, 3.0864]) [3.09, 2.73, 3.18]\n",
      "9300 46% (141m 21s) 0.0255 Karnes had his own J ... / tensor([3.0524, 3.1540, 2.9767]) [3.18, 2.91, 3.0]\n",
      "9400 47% (142m 53s) 0.0054 For information abou ... / tensor([3.0259, 2.9607, 2.9819]) [3.0, 3.0, 3.1]\n",
      "9500 47% (144m 24s) 0.3877 Even the city feels  ... / tensor([3.0158, 3.1729, 3.1064]) [3.8, 3.8, 3.5]\n",
      "9600 48% (145m 53s) 0.0394 Unfortunately, this  ... / tensor([3.0321, 3.0247, 3.0331]) [2.71, 3.14, 3.0]\n",
      "9700 48% (147m 30s) 0.0170 These collectives ar ... / tensor([3.1869, 2.9304, 3.0510]) [3.2, 3.1, 3.2]\n",
      "9800 49% (149m 9s) 0.0679 Sincerely, / tensor([2.9285, 2.6855, 2.8595]) [3.22, 3.0, 3.0]\n",
      "9900 49% (150m 31s) 0.2329 'Lost's' Fox: 'I'm a ... / tensor([2.2188, 2.7157, 2.6445]) [2.0, 3.44, 3.0]\n",
      "10000 50% (152m 11s) 0.0902 \"What?\" / tensor([2.7909, 2.8860, 2.9386]) [3.1, 3.3, 3.0]\n",
      "10100 50% (153m 38s) 0.0149 August 4, 1999 / tensor([3.0466, 2.7999, 2.9401]) [3.0, 3.0, 2.89]\n",
      "10200 51% (155m 26s) 0.0630 One of the most inte ... / tensor([2.9844, 3.0244, 3.0196]) [3.22, 3.33, 3.22]\n",
      "10300 51% (157m 11s) 0.2717 For some reason I th ... / tensor([2.4485, 3.0067, 2.8614]) [2.1, 3.7, 2.4]\n",
      "10400 52% (158m 37s) 0.0211 Please use the enclo ... / tensor([3.3231, 3.5674, 3.2640]) [3.5, 3.4, 3.2]\n",
      "10500 52% (160m 16s) 0.0747 Tinian Island was th ... / tensor([2.6967, 2.9919, 2.9007]) [2.89, 2.56, 2.89]\n",
      "10600 53% (161m 45s) 0.0022 said Zheng. / tensor([2.9435, 2.7773, 3.0789]) [2.88, 2.75, 3.12]\n",
      "10700 53% (163m 17s) 0.0270 Federer overcomes a  ... / tensor([2.7569, 3.1389, 3.0476]) [3.0, 3.0, 3.1]\n",
      "10800 54% (164m 39s) 0.0975 Sunshine melted lazi ... / tensor([3.0406, 3.1013, 3.0670]) [3.5, 3.0, 2.8]\n",
      "10900 54% (166m 19s) 0.0379 Your gift to Goodwil ... / tensor([3.4360, 3.1253, 3.1242]) [3.63, 3.25, 3.37]\n",
      "11000 55% (167m 53s) 0.0832 I don't know. / tensor([2.7177, 3.1020, 3.0176]) [2.89, 3.0, 2.56]\n",
      "11100 55% (169m 18s) 0.0438 Starke said the scho ... / tensor([2.6850, 2.9805, 2.9398]) [3.0, 2.9, 3.1]\n",
      "11200 56% (170m 44s) 0.0496 “Nathan.” / tensor([2.9812, 3.0133, 3.1578]) [3.0, 2.63, 3.12]\n",
      "11300 56% (172m 24s) 0.1225 \"I would not have th ... / tensor([2.8300, 3.0972, 3.0239]) [3.11, 2.56, 3.0]\n",
      "11400 56% (174m 12s) 0.0617 By accepting my pers ... / tensor([3.2754, 3.1167, 3.1509]) [3.57, 3.43, 3.14]\n",
      "11500 57% (175m 50s) 0.0913 A mass of flaming wo ... / tensor([3.0158, 3.2529, 3.0248]) [2.67, 3.56, 2.78]\n",
      "11600 57% (177m 21s) 0.0107 Factories in Kowloon ... / tensor([3.3501, 3.0288, 3.1144]) [3.4, 3.2, 3.1]\n",
      "11700 58% (178m 56s) 0.1271 In advance, please a ... / tensor([3.5409, 3.5912, 3.2826]) [4.0, 3.29, 3.0]\n",
      "11800 59% (180m 29s) 0.1206 It can open doors to ... / tensor([3.1974, 3.1605, 3.0065]) [3.5, 3.62, 3.25]\n",
      "11900 59% (181m 57s) 0.0716 What next, will the  ... / tensor([2.7223, 3.0635, 3.0352]) [2.56, 3.22, 3.44]\n",
      "12000 60% (183m 19s) 0.1695 Tasha and I became l ... / tensor([3.2675, 3.2402, 3.1099]) [3.7, 3.8, 3.2]\n",
      "12100 60% (184m 49s) 0.0126 Sincerely, / tensor([2.8854, 2.6967, 2.9173]) [3.0, 2.83, 3.0]\n",
      "12200 61% (186m 29s) 0.0827 Listen to loons. / tensor([3.0108, 3.0590, 3.0934]) [3.0, 2.6, 2.9]\n",
      "12300 61% (188m 0s) 0.0039 Eventually, he turns ... / tensor([3.0318, 3.0564, 3.1697]) [3.11, 3.0, 3.22]\n",
      "12400 62% (189m 27s) 0.0574 Andrew Russell to lo ... / tensor([3.2184, 3.0207, 3.2469]) [3.0, 2.7, 3.1]\n",
      "12500 62% (191m 1s) 0.0079 The National Associa ... / tensor([3.0482, 2.9948, 3.0744]) [3.2, 3.0, 3.1]\n",
      "12600 63% (192m 30s) 0.0442 but it is in fact th ... / tensor([3.2652, 2.8943, 3.0753]) [3.1, 2.9, 3.4]\n",
      "12700 63% (193m 58s) 0.0014 I want to see what a ... / tensor([2.9830, 3.0866, 3.0308]) [3.0, 3.14, 3.0]\n",
      "12800 64% (195m 30s) 0.3540 Toddler died from E. ... / tensor([2.1732, 2.8806, 2.7502]) [1.62, 3.75, 2.75]\n",
      "12900 64% (197m 10s) 0.0291 \"What is this?\" / tensor([2.9029, 3.0157, 2.9250]) [2.89, 3.0, 3.22]\n",
      "13000 65% (198m 34s) 0.0009 Vive la nouveau Sono ... / tensor([3.2525, 2.8956, 3.1025]) [3.2, 2.9, 3.1]\n",
      "13100 65% (200m 2s) 0.1050 Let’s go together, y ... / tensor([3.1594, 3.1268, 3.0924]) [3.56, 3.44, 3.33]\n",
      "13200 66% (201m 39s) 0.0553 If I know him, I can ... / tensor([3.0712, 3.2192, 3.1752]) [3.0, 3.29, 3.57]\n",
      "13300 66% (203m 17s) 0.0202 The oral nature of j ... / tensor([2.9977, 2.8409, 3.0483]) [3.0, 2.6, 3.1]\n",
      "13400 67% (204m 47s) 0.0908 Today, I kept it sim ... / tensor([3.0026, 3.0055, 2.8844]) [3.3, 2.9, 3.3]\n",
      "13500 67% (206m 14s) 0.0076 These gardens used t ... / tensor([2.9787, 2.9765, 3.0616]) [2.88, 2.88, 3.0]\n",
      "13600 68% (207m 43s) 0.0438 Audubon establishes  ... / tensor([3.1612, 3.0296, 3.1566]) [3.46, 3.0, 3.36]\n",
      "13700 68% (209m 27s) 0.0440 She disagreed with t ... / tensor([3.0520, 3.0579, 3.0556]) [2.8, 2.8, 3.1]\n",
      "13800 69% (211m 1s) 0.0152 Breakup rings find n ... / tensor([2.8174, 3.0166, 2.8908]) [3.0, 3.0, 3.0]\n",
      "13900 69% (212m 36s) 0.0440 That is especially t ... / tensor([2.7481, 3.0245, 2.9948]) [3.0, 2.83, 3.17]\n",
      "14000 70% (214m 8s) 0.0050 Now, Margaret Seltze ... / tensor([2.8643, 3.1250, 3.2007]) [2.8, 3.1, 3.1]\n",
      "14100 70% (215m 48s) 0.0404 Was Prince's super s ... / tensor([3.0809, 3.0890, 3.0312]) [2.88, 3.13, 2.75]\n",
      "14200 71% (217m 18s) 0.1993 Although for other c ... / tensor([2.7135, 2.9445, 3.1009]) [2.43, 3.43, 2.57]\n",
      "14300 71% (218m 43s) 0.0918 Google executive act ... / tensor([2.9473, 2.9436, 3.1303]) [3.4, 3.2, 3.2]\n",
      "14400 72% (220m 11s) 0.0040 It is Moore's way to ... / tensor([3.1077, 3.1340, 3.1290]) [3.0, 3.12, 3.12]\n",
      "14500 72% (221m 38s) 0.0023 In fact, Moore menti ... / tensor([3.0650, 3.0455, 3.1258]) [3.0, 3.0, 3.1]\n",
      "14600 73% (223m 3s) 0.0385 Beyond, in the New T ... / tensor([3.0850, 2.7775, 3.0307]) [3.0, 2.45, 3.0]\n",
      "14700 73% (224m 35s) 0.0201 “I wouldn't miss thi ... / tensor([2.7774, 3.0789, 3.1043]) [2.8, 3.3, 3.0]\n",
      "14800 74% (226m 20s) 0.0683 As I did, I noticed  ... / tensor([2.8600, 3.1483, 2.8593]) [2.78, 2.78, 3.11]\n",
      "14900 74% (228m 0s) 0.0055 “This is someone fro ... / tensor([3.1192, 2.9894, 3.0287]) [3.11, 2.89, 3.11]\n",
      "15000 75% (229m 35s) 0.0141 Tell her...\" / tensor([3.0385, 3.2218, 3.0907]) [2.9, 3.1, 3.0]\n",
      "15100 75% (231m 6s) 0.0072 Roddick overcomes Fe ... / tensor([3.1136, 3.3162, 3.1739]) [3.2, 3.2, 3.2]\n",
      "15200 76% (232m 37s) 0.0032 Because the same gen ... / tensor([3.0141, 2.8436, 3.1129]) [3.0, 2.8, 3.2]\n",
      "15300 76% (234m 0s) 0.0212 Then, at Steve's sit ... / tensor([3.1827, 2.9483, 3.1660]) [3.0, 3.0, 3.0]\n",
      "15400 77% (235m 39s) 0.0365 So what does this ha ... / tensor([3.2015, 3.0308, 3.1368]) [2.9, 2.9, 3.1]\n",
      "15500 77% (237m 9s) 0.0183 Additionally, a hand ... / tensor([3.1568, 2.9299, 3.1260]) [3.3, 3.1, 3.2]\n",
      "15600 78% (238m 36s) 0.0377 For serious antiques ... / tensor([3.1954, 2.8345, 2.9818]) [3.0, 3.0, 3.2]\n",
      "15700 78% (239m 57s) 0.0518 he was already there ... / tensor([2.9712, 2.9871, 2.9141]) [3.0, 2.63, 2.75]\n",
      "15800 79% (241m 27s) 0.0267 Standing in the cent ... / tensor([3.0053, 3.0150, 3.0381]) [2.8, 3.2, 3.1]\n",
      "15900 79% (242m 45s) 0.0918 When I was done ment ... / tensor([2.8831, 3.0171, 2.8931]) [2.7, 3.5, 2.8]\n",
      "16000 80% (244m 15s) 0.0140 Inorganic nature has ... / tensor([3.0988, 2.9329, 3.0333]) [3.0, 3.0, 3.2]\n",
      "16100 80% (245m 46s) 0.0156 The classic Chinese  ... / tensor([3.1913, 3.0994, 3.1569]) [3.4, 3.1, 3.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16200 81% (247m 28s) 0.0184 On its front gripper ... / tensor([2.9707, 3.0070, 3.0543]) [3.0, 2.88, 3.25]\n",
      "16300 81% (248m 55s) 0.0825 Kitsch is vicarious  ... / tensor([2.8504, 3.1434, 2.9833]) [2.5, 2.8, 2.9]\n",
      "16400 82% (250m 29s) 0.3574 That is potentially  ... / tensor([3.2622, 2.9783, 3.1271]) [2.27, 3.18, 2.91]\n",
      "16500 82% (252m 6s) 0.0166 Surrounded by acres  ... / tensor([3.1350, 2.9494, 3.0959]) [3.0, 2.8, 3.0]\n",
      "16600 83% (253m 41s) 0.0198 But to the thousands ... / tensor([2.8274, 2.9354, 3.0600]) [2.6, 3.0, 3.0]\n",
      "16700 83% (255m 20s) 0.0118 Zheng, short and dar ... / tensor([2.9895, 2.9566, 2.9485]) [3.1, 3.1, 3.0]\n",
      "16800 84% (256m 53s) 0.0240 She reached for anot ... / tensor([2.8823, 2.9299, 2.9592]) [3.0, 2.75, 3.12]\n",
      "16900 84% (258m 22s) 0.0934 With your increased  ... / tensor([2.9145, 2.9102, 3.0236]) [3.1, 3.4, 3.1]\n",
      "17000 85% (259m 46s) 0.0707 \"Oh?\" / tensor([2.8717, 3.0300, 3.0271]) [2.9, 3.4, 3.3]\n",
      "17100 85% (261m 14s) 0.0110 Because the same gen ... / tensor([3.0784, 2.9594, 3.1620]) [3.0, 2.8, 3.2]\n",
      "17200 86% (262m 44s) 0.0452 The room held a bed, ... / tensor([3.1793, 3.0335, 3.0795]) [2.86, 2.86, 3.14]\n",
      "17300 86% (264m 22s) 0.0154 For the play of ligh ... / tensor([3.2441, 2.9746, 3.2411]) [3.1, 2.9, 3.1]\n",
      "17400 87% (266m 4s) 0.0553 'Grey's,' 'Betty,' ' ... / tensor([3.2716, 3.1839, 3.0448]) [3.6, 3.0, 3.2]\n",
      "17500 87% (267m 27s) 0.0303 “Disappeared,” I cor ... / tensor([3.0046, 3.1809, 3.1676]) [2.9, 2.9, 3.2]\n",
      "17600 88% (268m 48s) 0.0934 Ex-Border Patrol age ... / tensor([2.5080, 3.1631, 2.9409]) [2.2, 2.9, 2.6]\n",
      "17700 88% (270m 13s) 0.0282 Software giant SAP m ... / tensor([2.6058, 2.6329, 2.8563]) [2.5, 2.9, 2.9]\n",
      "17800 89% (271m 36s) 0.0553 All children who att ... / tensor([3.0304, 2.8617, 3.0708]) [2.92, 2.5, 2.92]\n",
      "17900 89% (273m 11s) 0.0512 Your contribution wi ... / tensor([3.3928, 2.8325, 3.1363]) [3.5, 3.12, 3.38]\n",
      "18000 90% (274m 54s) 0.2777 2. Successful comple ... / tensor([2.9259, 2.9466, 2.8072]) [3.5, 3.1, 3.5]\n",
      "18100 90% (276m 34s) 0.0203 Woods also thought t ... / tensor([2.7653, 2.8080, 2.8951]) [2.88, 3.0, 3.0]\n",
      "18200 91% (277m 57s) 0.0284 During times of cris ... / tensor([2.8453, 2.9135, 3.0159]) [2.9, 3.2, 3.0]\n",
      "18300 91% (279m 28s) 0.0667 \"What?\" / tensor([3.0460, 3.0356, 2.9827]) [2.8, 3.4, 2.9]\n",
      "18400 92% (280m 59s) 0.0198 \"What would it hurt? ... / tensor([2.7363, 3.2130, 2.9940]) [2.8, 3.1, 3.2]\n",
      "18500 92% (282m 38s) 0.0247 No, we're the start  ... / tensor([3.0010, 3.1703, 3.1491]) [2.9, 3.2, 3.4]\n",
      "18600 93% (284m 22s) 0.1033 I wanted to prove to ... / tensor([3.0251, 3.2050, 3.0374]) [3.12, 3.5, 3.5]\n",
      "18700 93% (286m 0s) 0.1447 Hackers strike at ke ... / tensor([2.4186, 2.8845, 2.8066]) [2.0, 3.3, 3.1]\n",
      "18800 94% (287m 37s) 0.0252 ‘What did you say?’ / tensor([2.7506, 3.1332, 2.9060]) [3.0, 3.2, 3.0]\n",
      "18900 94% (289m 10s) 0.1540 What's really bother ... / tensor([3.0468, 3.2506, 3.1438]) [2.6, 2.8, 2.9]\n",
      "19000 95% (290m 33s) 0.0113 In a brief speech, S ... / tensor([2.9432, 2.8871, 3.0595]) [3.1, 2.8, 3.1]\n",
      "19100 95% (292m 8s) 0.0241 The United States cu ... / tensor([2.9216, 3.0967, 3.2055]) [2.75, 3.12, 3.0]\n",
      "19200 96% (293m 49s) 0.0461 OK, watch and rememb ... / tensor([3.1158, 3.3251, 3.1085]) [3.14, 3.14, 3.43]\n",
      "19300 96% (295m 15s) 0.1828 Foetal mechanism hel ... / tensor([2.9048, 3.3491, 3.0454]) [3.6, 3.1, 3.1]\n",
      "19400 97% (296m 45s) 0.1597 Lies, damned lies, a ... / tensor([2.5655, 3.3150, 2.9568]) [1.9, 3.5, 3.0]\n",
      "19500 97% (298m 16s) 0.0347 It is paradoxical no ... / tensor([2.8902, 2.9038, 3.1273]) [2.91, 3.0, 2.82]\n",
      "19600 98% (299m 37s) 0.0028 \"I had a friend, the ... / tensor([3.0153, 2.8648, 3.0079]) [3.1, 2.9, 3.0]\n",
      "19700 98% (301m 2s) 0.0226 All three are void a ... / tensor([2.8795, 2.8780, 3.1630]) [2.7, 2.7, 3.1]\n",
      "19800 99% (302m 30s) 0.0315 The boys came in qui ... / tensor([2.7289, 3.1332, 2.9433]) [2.7, 3.3, 3.2]\n",
      "19900 99% (304m 7s) 0.0302 There are fewer boat ... / tensor([2.9348, 2.9141, 2.9365]) [2.8, 2.7, 3.1]\n",
      "20000 100% (305m 36s) 0.1058 The command of a lan ... / tensor([3.0523, 2.9386, 3.0279]) [3.12, 2.38, 3.0]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "if load_pre_trained==False:\n",
    "    n_iters = 20000\n",
    "    print_every = 100\n",
    "    plot_every = 50\n",
    "\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    test_loss = []\n",
    "\n",
    "    def timeSince(since):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        sentence, VAD, tensor_sentence, tensor_VAD = randomTrainingExample()\n",
    "        output, loss = train(tensor_VAD, tensor_sentence)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100,  timeSince(start), loss, \n",
    "                                                    portionSentence(sentence), output.data[0], VAD))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "            test_loss.append(test_while_training())\n",
    "else:\n",
    "    rnn.load_state_dict(torch.load('C:\\\\Users\\\\fast\\\\camilo\\\\ST7\\\\rnn_VAD.pt'))\n",
    "    #with the corresponding correct path\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "if not load_pre_trained:\n",
    "    fig = plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    ax.legend()\n",
    "    ax.plot(all_losses,label='training loss')\n",
    "    ax.plot(test_loss,label='test loss')\n",
    "    plt.title('Loss while training')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_2_val, prediction_2_arr, prediction_2_dom = [], [], []\n",
    "for i in test_2.index:\n",
    "    try:\n",
    "        sentence = test_2['text'][i]\n",
    "        s_t = SentenceToTensor(sentence)\n",
    "        pred_val, pred_arr, pred_dom = evaluate(s_t)[0], evaluate(s_t)[1], evaluate(s_t)[2]\n",
    "        prediction_2_val.append(pred_val)\n",
    "        prediction_2_arr.append(pred_arr)\n",
    "        prediction_2_dom.append(pred_dom)\n",
    "    except:\n",
    "        prediction_2_val.append(3.)\n",
    "        prediction_2_arr.append(3.)\n",
    "        prediction_2_dom.append(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 error of approach 1 for valence = 0.2626469714832306\n",
      "L1 error of approach 1 for arrousal = 0.21056396457195284\n",
      "L1 error of approach 1 for dominance = 0.17682739300727848\n"
     ]
    }
   ],
   "source": [
    "Error_Val = np.absolute(prediction_2_val - test_2['V'].values).mean()\n",
    "print('L1 error of approach 1 for valence = '+str(Error_Val))\n",
    "Error_Val = np.absolute(prediction_2_arr - test_2['A'].values).mean()\n",
    "print('L1 error of approach 1 for arrousal = '+str(Error_Val))\n",
    "Error_Val = np.absolute(prediction_2_dom - test_2['D'].values).mean()\n",
    "print('L1 error of approach 1 for dominance = '+str(Error_Val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "When my father died, a year after my mother, my sisters and I cleaned out their apartment.\n",
      "Valence = 2.0, predicted valence = 2.72741436958313\n",
      "Arrousal = 2.78, predicted arrousal = 3.115412950515747\n",
      "Dominance = 2.0, predicted dominance = 3.0445940494537354\n",
      "--------------------------------------------------------------------------------------\n",
      "Outdated baby food found on shelves\n",
      "Valence = 1.89, predicted valence = 2.974700927734375\n",
      "Arrousal = 3.22, predicted arrousal = 3.196429491043091\n",
      "Dominance = 1.89, predicted dominance = 3.132033348083496\n",
      "--------------------------------------------------------------------------------------\n",
      "So empty it hurt.\n",
      "Valence = 2.0, predicted valence = 2.493443012237549\n",
      "Arrousal = 3.25, predicted arrousal = 3.032355785369873\n",
      "Dominance = 2.0, predicted dominance = 3.021170139312744\n",
      "--------------------------------------------------------------------------------------\n",
      "\"This is a criminal act and it certainly puts things in a different league,\"\n",
      "Valence = 2.0, predicted valence = 2.9761714935302734\n",
      "Arrousal = 3.1, predicted arrousal = 3.1321938037872314\n",
      "Dominance = 2.0, predicted dominance = 3.30340576171875\n"
     ]
    }
   ],
   "source": [
    "for i in [7518, 4949, 7066, 2947]:\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    sentence = test_2['text'][i]\n",
    "    s_t = SentenceToTensor(sentence)\n",
    "    pred_val, pred_arr, pred_dom = evaluate(s_t)[0], evaluate(s_t)[1], evaluate(s_t)[2]\n",
    "    val, arr, dom  = test_2['V'][i], test_2['A'][i], test_2['D'][i]\n",
    "    print(test_1['text'][i])\n",
    "    print('Valence = {}, predicted valence = {}'.format(val,pred_val))\n",
    "    print('Arrousal = {}, predicted arrousal = {}'.format(arr,pred_arr))\n",
    "    print('Dominance = {}, predicted dominance = {}'.format(val,pred_dom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "And the last lesson -- have fun .\n",
      "Valence = 4.0, predicted valence = 3.11696195602417\n",
      "Arrousal = 3.14, predicted arrousal = 3.094287157058716\n",
      "Dominance = 4.0, predicted dominance = 3.156259059906006\n",
      "--------------------------------------------------------------------------------------\n",
      "Canadian breakthrough offers hope on autism\n",
      "Valence = 3.88, predicted valence = 2.9939565658569336\n",
      "Arrousal = 3.5, predicted arrousal = 2.6970303058624268\n",
      "Dominance = 3.88, predicted dominance = 2.9542040824890137\n",
      "--------------------------------------------------------------------------------------\n",
      "We slammed against the doorway and I was laughing too, the pulse close enough to shake the doorframe and set up vibrations in my chest, Rachel in my arms because she’d used me to soften her landing.\n",
      "Valence = 4.1, predicted valence = 2.611011028289795\n",
      "Arrousal = 3.8, predicted arrousal = 2.972097396850586\n",
      "Dominance = 4.1, predicted dominance = 2.896580457687378\n",
      "--------------------------------------------------------------------------------------\n",
      "The proverbial hospitality and warm welcome are still here.\n",
      "Valence = 3.75, predicted valence = 3.0658459663391113\n",
      "Arrousal = 3.12, predicted arrousal = 2.9723870754241943\n",
      "Dominance = 3.75, predicted dominance = 3.0354204177856445\n"
     ]
    }
   ],
   "source": [
    "for i in [4455, 4618, 3871, 2613]:\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    sentence = test_2['text'][i]\n",
    "    s_t = SentenceToTensor(sentence)\n",
    "    pred_val, pred_arr, pred_dom = evaluate(s_t)[0], evaluate(s_t)[1], evaluate(s_t)[2]\n",
    "    val, arr, dom  = test_2['V'][i], test_2['A'][i], test_2['D'][i]\n",
    "    print(test_1['text'][i])\n",
    "    print('Valence = {}, predicted valence = {}'.format(val,pred_val))\n",
    "    print('Arrousal = {}, predicted arrousal = {}'.format(arr,pred_arr))\n",
    "    print('Dominance = {}, predicted dominance = {}'.format(val,pred_dom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input\n",
    "And then we can just try to predict any sentence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAD_sentence(sen):\n",
    "    print('--------------------------------------------------------------------------------------')\n",
    "    s_t = SentenceToTensor(sen)\n",
    "    val, arr, dom = evaluate(s_t)[0], evaluate(s_t)[1], evaluate(s_t)[2]\n",
    "    print('Evaluating the following sentence = '+sen)\n",
    "    print('Valence = {}, Arrousal = {}, Dominance = {}'.format(val,arr,dom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "Evaluating the following sentence = im tired of being locked in\n",
      "Valence = 2.068488597869873, Arrousal = 2.7384531497955322, Dominance = 2.434692859649658\n"
     ]
    }
   ],
   "source": [
    "#write your sentence here\n",
    "sentence = 'im tired of being locked in'\n",
    "VAD_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we turn these values into emotions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_pre_trained:\n",
    "    torch.save(rnn.state_dict(), 'C:\\\\Users\\\\fast\\\\camilo\\\\ST7\\\\rnn_VAD.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ST7)",
   "language": "python",
   "name": "st7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
